{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ba75ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "    WARNING ⚠️ stream/video/webcam/dir predict source will accumulate results in RAM unless `stream=True` is passed,\n",
      "    causing potential out-of-memory errors for large sources or long-running streams/videos.\n",
      "\n",
      "    Usage:\n",
      "        results = model(source=..., stream=True)  # generator of Results objects\n",
      "        for r in results:\n",
      "            boxes = r.boxes  # Boxes object for bbox outputs\n",
      "            masks = r.masks  # Masks object for segment masks outputs\n",
      "            probs = r.probs  # Class probabilities for classification outputs\n",
      "\n",
      "video 1/1 (1/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 3 persons, 54.8ms\n",
      "video 1/1 (2/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 3 persons, 3.2ms\n",
      "video 1/1 (3/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 4 persons, 2.8ms\n",
      "video 1/1 (4/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 4 persons, 2.8ms\n",
      "video 1/1 (5/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 4 persons, 2.9ms\n",
      "video 1/1 (6/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 4 persons, 2.8ms\n",
      "video 1/1 (7/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 4 persons, 2.8ms\n",
      "video 1/1 (8/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 4 persons, 6.0ms\n",
      "video 1/1 (9/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 4 persons, 2.8ms\n",
      "video 1/1 (10/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 4 persons, 2.8ms\n",
      "video 1/1 (11/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 4 persons, 2.9ms\n",
      "video 1/1 (12/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 4 persons, 2.8ms\n",
      "video 1/1 (13/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 4 persons, 2.9ms\n",
      "video 1/1 (14/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 4 persons, 2.8ms\n",
      "video 1/1 (15/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 4 persons, 2.8ms\n",
      "video 1/1 (16/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 4 persons, 2.8ms\n",
      "video 1/1 (17/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 4 persons, 2.8ms\n",
      "video 1/1 (18/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 4 persons, 2.8ms\n",
      "video 1/1 (19/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 4 persons, 2.8ms\n",
      "video 1/1 (20/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 4 persons, 2.8ms\n",
      "video 1/1 (21/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 4 persons, 2.8ms\n",
      "video 1/1 (22/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 4 persons, 2.8ms\n",
      "video 1/1 (23/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 4 persons, 2.8ms\n",
      "video 1/1 (24/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 4 persons, 2.8ms\n",
      "video 1/1 (25/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 4 persons, 2.7ms\n",
      "video 1/1 (26/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 4 persons, 2.8ms\n",
      "video 1/1 (27/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 3 persons, 2.8ms\n",
      "video 1/1 (28/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 3 persons, 2.8ms\n",
      "video 1/1 (29/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 3 persons, 3.0ms\n",
      "video 1/1 (30/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 3 persons, 2.8ms\n",
      "video 1/1 (31/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 3 persons, 2.8ms\n",
      "video 1/1 (32/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 3 persons, 2.9ms\n",
      "video 1/1 (33/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 3 persons, 2.8ms\n",
      "video 1/1 (34/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 3 persons, 2.8ms\n",
      "video 1/1 (35/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 3 persons, 2.8ms\n",
      "video 1/1 (36/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 3 persons, 2.8ms\n",
      "video 1/1 (37/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 3 persons, 2.8ms\n",
      "video 1/1 (38/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 3 persons, 2.8ms\n",
      "video 1/1 (39/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 3 persons, 2.8ms\n",
      "video 1/1 (40/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 3 persons, 2.8ms\n",
      "video 1/1 (41/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 3 persons, 2.8ms\n",
      "video 1/1 (42/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 3 persons, 2.7ms\n",
      "video 1/1 (43/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 3 persons, 2.8ms\n",
      "video 1/1 (44/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 3 persons, 2.8ms\n",
      "video 1/1 (45/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 3 persons, 2.8ms\n",
      "video 1/1 (46/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 3 persons, 2.7ms\n",
      "video 1/1 (47/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 4 persons, 2.7ms\n",
      "video 1/1 (48/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 4 persons, 2.7ms\n",
      "video 1/1 (49/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 4 persons, 2.8ms\n",
      "video 1/1 (50/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 4 persons, 2.9ms\n",
      "video 1/1 (51/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 4 persons, 2.8ms\n",
      "video 1/1 (52/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 4 persons, 2.8ms\n",
      "video 1/1 (53/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 4 persons, 2.8ms\n",
      "video 1/1 (54/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 4 persons, 2.8ms\n",
      "video 1/1 (55/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 4 persons, 2.8ms\n",
      "video 1/1 (56/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 4 persons, 2.8ms\n",
      "video 1/1 (57/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 4 persons, 2.8ms\n",
      "video 1/1 (58/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 4 persons, 2.8ms\n",
      "video 1/1 (59/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 4 persons, 2.8ms\n",
      "video 1/1 (60/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 4 persons, 3.0ms\n",
      "video 1/1 (61/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 4 persons, 2.8ms\n",
      "video 1/1 (62/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 4 persons, 2.8ms\n",
      "video 1/1 (63/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 3 persons, 2.8ms\n",
      "video 1/1 (64/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 3 persons, 2.8ms\n",
      "video 1/1 (65/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 3 persons, 2.8ms\n",
      "video 1/1 (66/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 3 persons, 2.8ms\n",
      "video 1/1 (67/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 3 persons, 2.8ms\n",
      "video 1/1 (68/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 3 persons, 2.8ms\n",
      "video 1/1 (69/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 3 persons, 2.8ms\n",
      "video 1/1 (70/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 3 persons, 3.0ms\n",
      "video 1/1 (71/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 3 persons, 2.8ms\n",
      "video 1/1 (72/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 3 persons, 2.8ms\n",
      "video 1/1 (73/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 3 persons, 2.8ms\n",
      "video 1/1 (74/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 3 persons, 2.9ms\n",
      "video 1/1 (75/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 3 persons, 2.8ms\n",
      "video 1/1 (76/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 3 persons, 2.8ms\n",
      "video 1/1 (77/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 3 persons, 2.8ms\n",
      "video 1/1 (78/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 3 persons, 2.8ms\n",
      "video 1/1 (79/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 3 persons, 2.9ms\n",
      "video 1/1 (80/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 3 persons, 2.9ms\n",
      "video 1/1 (81/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 3 persons, 2.7ms\n",
      "video 1/1 (82/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 3 persons, 2.8ms\n",
      "video 1/1 (83/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 3 persons, 2.7ms\n",
      "video 1/1 (84/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 3 persons, 2.8ms\n",
      "video 1/1 (85/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 3 persons, 2.8ms\n",
      "video 1/1 (86/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 3 persons, 2.8ms\n",
      "video 1/1 (87/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 3 persons, 2.8ms\n",
      "video 1/1 (88/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 3 persons, 3.0ms\n",
      "video 1/1 (89/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 3 persons, 2.9ms\n",
      "video 1/1 (90/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 3 persons, 2.7ms\n",
      "video 1/1 (91/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 3 persons, 2.7ms\n",
      "video 1/1 (92/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 3 persons, 2.7ms\n",
      "video 1/1 (93/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 3 persons, 2.8ms\n",
      "video 1/1 (94/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 3 persons, 2.9ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "video 1/1 (95/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 3 persons, 2.9ms\n",
      "video 1/1 (96/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 3 persons, 2.8ms\n",
      "video 1/1 (97/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 3 persons, 2.8ms\n",
      "video 1/1 (98/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 3 persons, 2.8ms\n",
      "video 1/1 (99/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 3 persons, 2.7ms\n",
      "video 1/1 (100/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 3 persons, 2.7ms\n",
      "video 1/1 (101/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 3 persons, 2.8ms\n",
      "video 1/1 (102/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 3 persons, 2.7ms\n",
      "video 1/1 (103/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 3 persons, 2.8ms\n",
      "video 1/1 (104/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 3 persons, 3.1ms\n",
      "video 1/1 (105/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 3 persons, 2.9ms\n",
      "video 1/1 (106/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 3 persons, 2.8ms\n",
      "video 1/1 (107/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 3 persons, 2.8ms\n",
      "video 1/1 (108/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 3 persons, 2.9ms\n",
      "video 1/1 (109/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 3 persons, 2.9ms\n",
      "video 1/1 (110/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 3 persons, 2.8ms\n",
      "video 1/1 (111/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 3 persons, 2.8ms\n",
      "video 1/1 (112/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 3 persons, 2.8ms\n",
      "video 1/1 (113/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 3 persons, 2.8ms\n",
      "video 1/1 (114/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 3 persons, 2.8ms\n",
      "video 1/1 (115/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 3 persons, 2.8ms\n",
      "video 1/1 (116/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 3 persons, 2.8ms\n",
      "video 1/1 (117/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 3 persons, 2.8ms\n",
      "video 1/1 (118/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 3 persons, 2.8ms\n",
      "video 1/1 (119/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 3 persons, 2.8ms\n",
      "video 1/1 (120/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 3 persons, 2.8ms\n",
      "video 1/1 (121/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 3 persons, 2.7ms\n",
      "video 1/1 (122/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 3 persons, 2.8ms\n",
      "video 1/1 (123/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 2 persons, 2.8ms\n",
      "video 1/1 (124/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 2 persons, 2.7ms\n",
      "video 1/1 (125/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 3 persons, 2.7ms\n",
      "video 1/1 (126/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 3 persons, 2.8ms\n",
      "video 1/1 (127/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 3 persons, 2.8ms\n",
      "video 1/1 (128/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 3 persons, 2.9ms\n",
      "video 1/1 (129/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 3 persons, 2.9ms\n",
      "video 1/1 (130/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 2 persons, 3.0ms\n",
      "video 1/1 (131/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 2 persons, 2.8ms\n",
      "video 1/1 (132/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 2 persons, 2.8ms\n",
      "video 1/1 (133/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 2 persons, 3.0ms\n",
      "video 1/1 (134/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 2 persons, 2.8ms\n",
      "video 1/1 (135/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 2 persons, 2.8ms\n",
      "video 1/1 (136/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 2 persons, 2.8ms\n",
      "video 1/1 (137/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 2 persons, 2.7ms\n",
      "video 1/1 (138/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 2 persons, 2.7ms\n",
      "video 1/1 (139/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 2 persons, 2.7ms\n",
      "video 1/1 (140/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 2 persons, 2.7ms\n",
      "video 1/1 (141/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 2 persons, 2.9ms\n",
      "video 1/1 (142/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 2 persons, 3.0ms\n",
      "video 1/1 (143/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 2 persons, 3.1ms\n",
      "video 1/1 (144/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 2 persons, 2.8ms\n",
      "video 1/1 (145/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 2 persons, 2.8ms\n",
      "video 1/1 (146/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 2 persons, 2.8ms\n",
      "video 1/1 (147/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 2 persons, 2.9ms\n",
      "video 1/1 (148/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 2 persons, 3.0ms\n",
      "video 1/1 (149/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 2 persons, 2.8ms\n",
      "video 1/1 (150/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 2 persons, 2.8ms\n",
      "video 1/1 (151/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 2 persons, 2.8ms\n",
      "video 1/1 (152/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 2 persons, 2.9ms\n",
      "video 1/1 (153/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 2 persons, 2.9ms\n",
      "video 1/1 (154/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 2 persons, 2.8ms\n",
      "video 1/1 (155/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 2 persons, 2.9ms\n",
      "video 1/1 (156/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 1 person, 2.8ms\n",
      "video 1/1 (157/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 1 person, 2.9ms\n",
      "video 1/1 (158/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 1 person, 2.7ms\n",
      "video 1/1 (159/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 1 person, 2.8ms\n",
      "video 1/1 (160/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 1 person, 2.8ms\n",
      "video 1/1 (161/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 1 person, 2.7ms\n",
      "video 1/1 (162/162) /home/muhlabws4/Belgeler/tkd/tkd.mp4: 384x640 1 person, 2.8ms\n",
      "Speed: 1.1ms preprocess, 3.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Results saved to \u001b[1mruns/pose/track5\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO('objDet2Best.pt')\n",
    "model = YOLO('yolov8n-seg.pt')\n",
    "model = YOLO('yolov8n-pose.pt')\n",
    "results = model.track(source=\"/home/muhlabws4/Belgeler/tkd/tkd.mp4\", conf=0.3, iou=0.5, save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76c9fa0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "(python:11283): GStreamer-CRITICAL **: 15:20:09.659: gst_element_make_from_uri: assertion 'gst_uri_is_valid (uri)' failed\n",
      "[ WARN:0@0.524] global /croot/opencv-suite_1676452025216/work/modules/videoio/src/cap_gstreamer.cpp (1127) open OpenCV | GStreamer warning: Error opening bin: no source element for URI \"/home/muhlabws4/dosyalar/runs/detect/predic6/tkd.avi\"\n",
      "[ WARN:0@0.524] global /croot/opencv-suite_1676452025216/work/modules/videoio/src/cap_gstreamer.cpp (862) isPipelinePlaying OpenCV | GStreamer warning: GStreamer: pipeline have not been created\n",
      "[ERROR:0@0.546] global /croot/opencv-suite_1676452025216/work/modules/videoio/src/cap.cpp (164) open VIDEOIO(CV_IMAGES): raised OpenCV exception:\n",
      "\n",
      "OpenCV(4.6.0) /croot/opencv-suite_1676452025216/work/modules/videoio/src/cap_images.cpp:253: error: (-5:Bad argument) CAP_IMAGES: can't find starting number (in the name of file): /home/muhlabws4/dosyalar/runs/detect/predic6/tkd.avi in function 'icvExtractPattern'\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Open the video file\n",
    "video_path = \"/home/muhlabws4/dosyalar/runs/detect/predic6/tkd.avi\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Loop through the video frames\n",
    "while cap.isOpened():\n",
    "    # Read a frame from the video\n",
    "    success, frame = cap.read()\n",
    "\n",
    "    if success:\n",
    "        # Run YOLOv8 tracking on the frame, persisting tracks between frames\n",
    "        results = model.track(frame, persist=True)\n",
    "\n",
    "        # Visualize the results on the frame\n",
    "        annotated_frame = results[0].plot()\n",
    "\n",
    "        # Display the annotated frame\n",
    "        cv2.imshow(\"YOLOv8 Tracking\", annotated_frame)\n",
    "\n",
    "        # Break the loop if 'q' is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "    else:\n",
    "        # Break the loop if the end of the video is reached\n",
    "        break\n",
    "\n",
    "# Release the video capture object and close the display window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2471bbd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 3 persons, 7.7ms\n",
      "Speed: 1.6ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 3.2ms\n",
      "Speed: 1.2ms preprocess, 3.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 3.2ms\n",
      "Speed: 1.2ms preprocess, 3.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 3.1ms\n",
      "Speed: 1.1ms preprocess, 3.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 3.0ms\n",
      "Speed: 1.1ms preprocess, 3.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 3.0ms\n",
      "Speed: 1.1ms preprocess, 3.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 3.0ms\n",
      "Speed: 1.1ms preprocess, 3.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 3.2ms\n",
      "Speed: 1.3ms preprocess, 3.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 3.1ms\n",
      "Speed: 1.1ms preprocess, 3.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 3.1ms\n",
      "Speed: 1.2ms preprocess, 3.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 3.1ms\n",
      "Speed: 1.2ms preprocess, 3.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 3.1ms\n",
      "Speed: 1.2ms preprocess, 3.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 3.3ms\n",
      "Speed: 1.2ms preprocess, 3.3ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 3.1ms\n",
      "Speed: 1.1ms preprocess, 3.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 3.0ms\n",
      "Speed: 1.1ms preprocess, 3.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 3.0ms\n",
      "Speed: 1.2ms preprocess, 3.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 3.0ms\n",
      "Speed: 1.1ms preprocess, 3.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 3.0ms\n",
      "Speed: 1.1ms preprocess, 3.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 3.1ms\n",
      "Speed: 1.2ms preprocess, 3.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 3.1ms\n",
      "Speed: 1.4ms preprocess, 3.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 3.1ms\n",
      "Speed: 1.1ms preprocess, 3.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 3.1ms\n",
      "Speed: 1.1ms preprocess, 3.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 3.0ms\n",
      "Speed: 1.2ms preprocess, 3.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 3.0ms\n",
      "Speed: 1.1ms preprocess, 3.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 3.1ms\n",
      "Speed: 1.1ms preprocess, 3.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 3.1ms\n",
      "Speed: 1.1ms preprocess, 3.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 3.1ms\n",
      "Speed: 1.1ms preprocess, 3.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 3.1ms\n",
      "Speed: 1.2ms preprocess, 3.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 3.1ms\n",
      "Speed: 1.2ms preprocess, 3.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 3.1ms\n",
      "Speed: 1.1ms preprocess, 3.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 3.1ms\n",
      "Speed: 1.2ms preprocess, 3.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 3.2ms\n",
      "Speed: 1.1ms preprocess, 3.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 3.0ms\n",
      "Speed: 1.1ms preprocess, 3.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 3.0ms\n",
      "Speed: 1.1ms preprocess, 3.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 3.1ms\n",
      "Speed: 1.1ms preprocess, 3.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 3.0ms\n",
      "Speed: 1.1ms preprocess, 3.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 3.2ms\n",
      "Speed: 1.2ms preprocess, 3.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 3.0ms\n",
      "Speed: 1.1ms preprocess, 3.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 3.1ms\n",
      "Speed: 1.2ms preprocess, 3.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 3.2ms\n",
      "Speed: 1.1ms preprocess, 3.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 3.1ms\n",
      "Speed: 1.1ms preprocess, 3.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 3.1ms\n",
      "Speed: 1.1ms preprocess, 3.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 3.0ms\n",
      "Speed: 1.1ms preprocess, 3.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 3.1ms\n",
      "Speed: 1.1ms preprocess, 3.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 3.1ms\n",
      "Speed: 1.3ms preprocess, 3.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 3.0ms\n",
      "Speed: 1.1ms preprocess, 3.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 3.2ms\n",
      "Speed: 1.2ms preprocess, 3.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 3.2ms\n",
      "Speed: 1.2ms preprocess, 3.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 3.1ms\n",
      "Speed: 1.1ms preprocess, 3.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 3.1ms\n",
      "Speed: 1.1ms preprocess, 3.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 3.2ms\n",
      "Speed: 1.1ms preprocess, 3.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 3.0ms\n",
      "Speed: 1.2ms preprocess, 3.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 3.0ms\n",
      "Speed: 1.2ms preprocess, 3.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 3.0ms\n",
      "Speed: 1.1ms preprocess, 3.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 3.1ms\n",
      "Speed: 1.2ms preprocess, 3.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 3.0ms\n",
      "Speed: 1.2ms preprocess, 3.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 3.2ms\n",
      "Speed: 1.1ms preprocess, 3.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 3.0ms\n",
      "Speed: 1.1ms preprocess, 3.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 3.0ms\n",
      "Speed: 1.2ms preprocess, 3.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 3.1ms\n",
      "Speed: 1.1ms preprocess, 3.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 3.1ms\n",
      "Speed: 1.2ms preprocess, 3.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 3.3ms\n",
      "Speed: 1.1ms preprocess, 3.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 3.1ms\n",
      "Speed: 1.1ms preprocess, 3.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 3.1ms\n",
      "Speed: 1.1ms preprocess, 3.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 3.0ms\n",
      "Speed: 1.1ms preprocess, 3.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 3.0ms\n",
      "Speed: 1.1ms preprocess, 3.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 3 persons, 3.0ms\n",
      "Speed: 1.1ms preprocess, 3.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 3.0ms\n",
      "Speed: 1.1ms preprocess, 3.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 3.0ms\n",
      "Speed: 1.1ms preprocess, 3.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 3.0ms\n",
      "Speed: 1.1ms preprocess, 3.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 3.0ms\n",
      "Speed: 1.1ms preprocess, 3.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 3.1ms\n",
      "Speed: 1.1ms preprocess, 3.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 3.1ms\n",
      "Speed: 1.1ms preprocess, 3.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 3.0ms\n",
      "Speed: 1.1ms preprocess, 3.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 3.0ms\n",
      "Speed: 1.1ms preprocess, 3.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 3.0ms\n",
      "Speed: 1.1ms preprocess, 3.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 3.1ms\n",
      "Speed: 1.3ms preprocess, 3.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 3.0ms\n",
      "Speed: 1.2ms preprocess, 3.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 3.0ms\n",
      "Speed: 1.2ms preprocess, 3.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 3.0ms\n",
      "Speed: 1.1ms preprocess, 3.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 3.0ms\n",
      "Speed: 1.1ms preprocess, 3.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 3.0ms\n",
      "Speed: 1.1ms preprocess, 3.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 3.1ms\n",
      "Speed: 1.1ms preprocess, 3.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 3.1ms\n",
      "Speed: 1.2ms preprocess, 3.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 3.1ms\n",
      "Speed: 1.3ms preprocess, 3.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 3.0ms\n",
      "Speed: 1.2ms preprocess, 3.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 3.0ms\n",
      "Speed: 1.2ms preprocess, 3.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 3.1ms\n",
      "Speed: 1.1ms preprocess, 3.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 3.1ms\n",
      "Speed: 1.2ms preprocess, 3.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 3.0ms\n",
      "Speed: 1.2ms preprocess, 3.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 3.0ms\n",
      "Speed: 1.1ms preprocess, 3.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 3.0ms\n",
      "Speed: 1.1ms preprocess, 3.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 2.9ms\n",
      "Speed: 1.2ms preprocess, 2.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 3.0ms\n",
      "Speed: 1.2ms preprocess, 3.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 3.0ms\n",
      "Speed: 1.1ms preprocess, 3.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 3.0ms\n",
      "Speed: 1.1ms preprocess, 3.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 3.1ms\n",
      "Speed: 1.1ms preprocess, 3.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 3.0ms\n",
      "Speed: 1.2ms preprocess, 3.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 3.1ms\n",
      "Speed: 1.1ms preprocess, 3.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 3.0ms\n",
      "Speed: 1.2ms preprocess, 3.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 3.0ms\n",
      "Speed: 1.1ms preprocess, 3.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 3.0ms\n",
      "Speed: 1.2ms preprocess, 3.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 3.0ms\n",
      "Speed: 1.2ms preprocess, 3.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 3.1ms\n",
      "Speed: 1.2ms preprocess, 3.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 3.3ms\n",
      "Speed: 1.2ms preprocess, 3.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 3.0ms\n",
      "Speed: 1.2ms preprocess, 3.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 3.2ms\n",
      "Speed: 1.2ms preprocess, 3.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 3.1ms\n",
      "Speed: 1.1ms preprocess, 3.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 3.0ms\n",
      "Speed: 1.1ms preprocess, 3.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 3.0ms\n",
      "Speed: 1.1ms preprocess, 3.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 3.0ms\n",
      "Speed: 1.1ms preprocess, 3.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 3.0ms\n",
      "Speed: 1.1ms preprocess, 3.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 3.0ms\n",
      "Speed: 1.2ms preprocess, 3.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 3.0ms\n",
      "Speed: 1.1ms preprocess, 3.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 3.1ms\n",
      "Speed: 1.3ms preprocess, 3.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 3.0ms\n",
      "Speed: 1.2ms preprocess, 3.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 3.0ms\n",
      "Speed: 1.2ms preprocess, 3.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 3.1ms\n",
      "Speed: 1.2ms preprocess, 3.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 3.1ms\n",
      "Speed: 1.2ms preprocess, 3.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 3.3ms\n",
      "Speed: 1.1ms preprocess, 3.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 3.0ms\n",
      "Speed: 1.1ms preprocess, 3.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 3.0ms\n",
      "Speed: 1.1ms preprocess, 3.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 3.1ms\n",
      "Speed: 1.1ms preprocess, 3.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 3.1ms\n",
      "Speed: 1.1ms preprocess, 3.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 3.0ms\n",
      "Speed: 1.1ms preprocess, 3.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 3.0ms\n",
      "Speed: 1.2ms preprocess, 3.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 3.0ms\n",
      "Speed: 1.2ms preprocess, 3.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 3.1ms\n",
      "Speed: 1.1ms preprocess, 3.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 3.1ms\n",
      "Speed: 1.1ms preprocess, 3.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 3.1ms\n",
      "Speed: 1.2ms preprocess, 3.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 3.0ms\n",
      "Speed: 1.1ms preprocess, 3.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 3.1ms\n",
      "Speed: 1.1ms preprocess, 3.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 3.2ms\n",
      "Speed: 1.2ms preprocess, 3.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 3.0ms\n",
      "Speed: 1.1ms preprocess, 3.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 3.0ms\n",
      "Speed: 1.1ms preprocess, 3.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 3.2ms\n",
      "Speed: 1.1ms preprocess, 3.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 3.0ms\n",
      "Speed: 1.1ms preprocess, 3.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 3.0ms\n",
      "Speed: 1.1ms preprocess, 3.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 3.1ms\n",
      "Speed: 1.1ms preprocess, 3.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 3.0ms\n",
      "Speed: 1.1ms preprocess, 3.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 3.1ms\n",
      "Speed: 1.1ms preprocess, 3.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 3.0ms\n",
      "Speed: 1.1ms preprocess, 3.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 3.0ms\n",
      "Speed: 1.2ms preprocess, 3.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 3.2ms\n",
      "Speed: 1.1ms preprocess, 3.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 3.0ms\n",
      "Speed: 1.1ms preprocess, 3.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 3.0ms\n",
      "Speed: 1.2ms preprocess, 3.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 3.1ms\n",
      "Speed: 1.1ms preprocess, 3.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from collections import defaultdict\n",
    "#from ultralytics import YOLO\n",
    "import numpy as np\n",
    "# Load the YOLOv8 model\n",
    "model1 = YOLO('yolov8n.pt')\n",
    "model1 = YOLO('yolov8n-seg.pt')\n",
    "model1 = YOLO('yolov8n-pose.pt')\n",
    "# Open the video file\n",
    "video_path = \"/home/muhlabws4/Belgeler/tkd/tkd.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "track_history = defaultdict(lambda: [])\n",
    "\n",
    "# Loop through the video frames\n",
    "while cap.isOpened():\n",
    "    # Read a frame from the video\n",
    "    success, frame = cap.read()\n",
    "\n",
    "    if success:\n",
    "        # Run YOLOv8 tracking on the frame, persisting tracks between frames\n",
    "        results = model.track(frame, persist=True)\n",
    "\n",
    "        # Get the boxes and track IDs\n",
    "        boxes = results[0].boxes.xywh.cpu()\n",
    "        track_ids = results[0].boxes.id.int().cpu().tolist()\n",
    "\n",
    "        # Visualize the results on the frame\n",
    "        annotated_frame = results[0].plot()\n",
    "\n",
    "        # Plot the tracks\n",
    "        for box, track_id in zip(boxes, track_ids):\n",
    "            x, y, w, h = box\n",
    "            track = track_history[track_id]\n",
    "            track.append((float(x), float(y)))  # x, y center point\n",
    "            if len(track) > 30:  # retain 90 tracks for 90 frames\n",
    "                track.pop(0)\n",
    "\n",
    "            # Draw the tracking lines\n",
    "            points = np.hstack(track).astype(np.int32).reshape((-1, 1, 2))\n",
    "            cv2.polylines(annotated_frame, [points], isClosed=False, color=(230, 230, 0), thickness=3)\n",
    "\n",
    "        # Display the annotated frame\n",
    "        cv2.imshow(\"YOLOv8 Tracking\", annotated_frame)\n",
    "\n",
    "        # Break the loop if 'q' is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "    else:\n",
    "        # Break the loop if the end of the video is reached\n",
    "        break\n",
    "\n",
    "# Release the video capture object and close the display window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200d556a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
